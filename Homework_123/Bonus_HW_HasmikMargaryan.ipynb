{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouC6teuHmxmQ"
      },
      "source": [
        "ÕÕ¿Õ¸Ö€Ö‡ Õ£Ö€Õ¾Õ¡Õ® Õ¡Õ¼Õ¡Õ»Õ¡Õ¤Ö€Õ¡Õ¶Ö„Õ¨ Õ¯Õ¡Õ¿Õ¡Ö€Õ¥Õ¬Õ¸Ö‚ Õ¾Õ¥Ö€Õ»Õ¶Õ¡ÕªÕ¡Õ´Õ¯Õ¥Õ¿Õ¨ **13.05.2023 23:59** Õ§Ö‰ Ô±Õ¼Õ¡Õ»Õ¡Õ¤Ö€Õ¡Õ¶Ö„Õ¨ Õ¯Õ¡Õ¿Õ¡Ö€Õ¥Õ¬Õ¸Ö‚Ö Õ°Õ¥Õ¿Õ¸, ÕºÕ¥Õ¿Ö„ Õ§ \n",
        "\n",
        "1. Õ¶Õ¥Ö€Õ¢Õ¥Õ¼Õ¶Õ¥Ö„ Õ¡ÕµÕ½ Ö†Õ¡ÕµÕ¬Õ¨ Õ±Õ¥Ö€ Õ°Õ¡Õ´Õ¡Õ¯Õ¡Ö€Õ£Õ«Õ¹ (`File` $\\to$ `Download .ipynb`)\n",
        "2. Õ¾Õ¥Ö€Õ¶Õ¡Õ£Ö€Õ«Õ¶ Õ¡Õ¾Õ¥Õ¬Õ¡ÖÕ¶Õ¥Ö„ Õ±Õ¥Ö€ Õ¡Õ¶Õ¸Ö‚Õ¶, Õ¡Õ¦Õ£Õ¡Õ¶Õ¸Ö‚Õ¶Õ¨ Õ¬Õ¡Õ¿Õ«Õ¶Õ¡Õ¿Õ¡Õ¼, Ö…Ö€Õ«Õ¶Õ¡Õ¯Õ *Bonus_HW_NshanPotikyan.ipynb* Ö†Õ¸Ö€Õ´Õ¡Õ¿Õ¸Õ¾\n",
        "3. Õ¸Ö‚Õ²Õ¡Ö€Õ¯Õ¥Ö„ Ö†Õ¡ÕµÕ¬Õ¨ **nshan.potikyan@gmail.com** Õ°Õ¡Õ½ÖÕ¥Õ«Õ¶` Õ¶Õ¡Õ´Õ¡Õ¯Õ« Õ©Õ¥Õ´Õ¡ (subject) Õ¤Õ¡Õ·Õ¿Õ¸Ö‚Õ´ Õ£Ö€Õ¥Õ¬Õ¸Õ¾ **Bonus**\n",
        "\n",
        "**ÕˆÖ‚Õ·Õ¡Õ¤Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶**â€¤ \n",
        "1. ÕŽÕ¥Ö€Õ¸Õ¶Õ·ÕµÕ¡Õ¬ ÕºÕ¡ÕµÕ´Õ¡Õ¶Õ¶Õ¥Ö€Õ«Õ¶ Õ¹Õ¢Õ¡Õ¾Õ¡Ö€Õ¡Ö€Õ¥Õ¬Õ¸Ö‚ Õ¤Õ¥ÕºÖ„Õ¸Ö‚Õ´ Õ±Õ¥Ö€ Õ¡Õ·Õ­Õ¡Õ¿Õ¡Õ¶Ö„Õ¨ Õ¹Õ« Õ£Õ¶Õ¡Õ°Õ¡Õ¿Õ¾Õ«Ö‰\n",
        "2. **numpy**, **pandas**, **sklearn** Õ£Ö€Õ¡Õ¤Õ¡Ö€Õ¡Õ¶Õ¶Õ¥Ö€Õ«Ö Õ¢Õ¡ÖÕ« Õ¡ÕµÕ¬ Õ£Ö€Õ¡Õ¤Õ¡Ö€Õ¡Õ¶Õ¶Õ¥Ö€Õ«Ö Õ¹Õ«Õ› Õ¯Õ¡Ö€Õ¥Õ¬Õ« Ö…Õ£Õ¿Õ¾Õ¥Õ¬ \n",
        "3. Ô¿Õ¸Õ¤Õ« Õ°Õ¡Õ´Õ¡Ö€ Õ¶Õ¡Õ­Õ¡Õ¿Õ¥Õ½Õ¾Õ¡Õ® Õ°Õ¡Õ¾Õ¥Õ¬ÕµÕ¡Õ¬ Õ¾Õ¡Õ¶Õ¤Õ¡Õ¯Õ¶Õ¥Ö€ Õ½Õ¿Õ¥Õ²Õ®Õ¥Õ¬, Õ«Õ¶Õ¹ÕºÕ¥Õ½ Õ¶Õ¡Ö‡ Õ¥Õ²Õ¡Õ® Õ¾Õ¡Õ¶Õ¤Õ¡Õ¯Õ¶Õ¥Ö€Õ« Õ°Õ¥Ö€Õ©Õ¡Õ¯Õ¡Õ¶Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨ ÖƒÕ¸Õ­Õ¥Õ¬ Õ¹Õ«Õ› Õ¯Õ¡Ö€Õ¥Õ¬Õ«  \n",
        "4. Ô¿Ö€Õ¯Õ¶Ö…Ö€Õ«Õ¶Õ¡Õ¯ Õ¬Õ¸Ö‚Õ®Õ¸Ö‚Õ´Õ¶Õ¥Ö€Õ« Õ°Õ¡Õ´Õ¡Ö€ Õ¯Õ½Õ¿Õ¡Õ¶Õ¡Ö„ 0 Õ´Õ«Õ¡Õ¾Õ¸Ö€Ö‰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Õ€Õ¡Õ¾Õ¥Õ¬ÕµÕ¡Õ¬ (ÕƒÕ«Õ·Õ¿ Õ¬Õ¸Ö‚Õ®Õ¥Õ¬Õ¸Ö‚ Õ¤Õ¥ÕºÖ„Õ¸Ö‚Õ´ Õ¯Õ½Õ¿Õ¡Õ¶Õ¡Ö„ 1/20 Õ´Õ«Õ¡Õ¾Õ¸Ö€)** \n",
        "\n",
        "Ô´Õ«ÖÕ¸Ö‚Ö„Õ ÖÕ¡Õ¶Õ¯Õ¡Õ¶Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ¯Õ¡Õ¼Õ¸Ö‚ÖÕ¥Õ¬ Õ¡ÕµÕ¶ÕºÕ«Õ½Õ« Õ¤Õ¡Õ½Õ¡Õ¯Õ¡Ö€Õ£Õ«Õ¹ (classifier), Õ¸Ö€Õ¨ Õ°Õ¶Õ¡Ö€Õ¡Õ¾Õ¸Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶ Õ¯Õ¿Õ¡ ÕµÕ¸Ö‚Ö€Õ¡Ö„Õ¡Õ¶Õ¹ÕµÕ¸Ö‚Ö€ Õ¸Ö‚Õ½Õ¡Õ¶Õ¸Õ²Õ« Õ°Õ¡Õ´Õ¡Ö€ Õ£Õ¶Õ¡Õ°Õ¡Õ¿Õ¥Õ¬ Õ¶Ö€Õ¡ Õ¯Õ¸Õ²Õ´Õ«Ö Õ«Õ¶Õ¹ÖŠÕ¸Ö€ Õ°Õ¡Ö€ÖÕ« Õ³Õ«Õ·Õ¿ ÕºÕ¡Õ¿Õ¡Õ½Õ­Õ¡Õ¶Õ¥Õ¬Õ¸Ö‚ Õ°Õ¡Õ¾Õ¡Õ¶Õ¡Õ¯Õ¡Õ¶Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨Õ Õ°Õ¡Õ·Õ¾Õ« Õ¡Õ¼Õ¶Õ¥Õ¬Õ¸Õ¾ Õ¡ÕµÕ¤ Õ¸Ö‚Õ½Õ¡Õ¶Õ¸Õ²Õ« Õ¿Õ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ¨ Ö‡ Õ°Õ¡Ö€ÖÕ« Õ¢Õ¡Ö€Õ¤Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨Ö‰ \n",
        "\n",
        "ÕˆÖ‚Õ½Õ¡Õ¶Õ¸Õ²Õ¶Õ¥Ö€Õ« Õ¿Õ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ«Ö Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¾Õ¥Õ¬Õ¸Ö‚ Õ§ Õ¶Ö€Õ¡ \n",
        "* Õ½Õ¥Õ¼Õ¨ (gender),\n",
        "* Õ¶Õ¡Õ­Õ¸Ö€Õ¤ Õ°Õ¡Ö€ÖÕ¥Ö€Õ«Õ¶ ÕºÕ¡Õ¿Õ¡Õ½Õ­Õ¡Õ¶Õ¥Õ¬Õ¸Ö‚ Õ´Õ«Õ»Õ«Õ¶ ÕªÕ¡Õ´Õ¡Õ¶Õ¡Õ¯Õ¨ (avg_time), \n",
        "* Õ¶Õ¡Õ­Õ¸Ö€Õ¤ Õ°Õ¡Ö€ÖÕ¥Ö€Õ«Õ¶ Õ³Õ«Õ·Õ¿ ÕºÕ¡Õ¿Õ¡Õ½Õ­Õ¡Õ¶Õ¥Õ¬Õ¸Ö‚ Õ´Õ«Õ»Õ«Õ¶ Õ°Õ¡Õ´Õ¡Õ´Õ¡Õ½Õ¶Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨ (avg_correct) \n",
        "\n",
        "ÕˆÖ€ÕºÕ¥Õ½ Õ°Õ¡Ö€ÖÕ« Õ¢Õ¡Ö€Õ¤Õ¸Ö‚Õ©ÕµÕ¡Õ¶ Õ¶Õ¯Õ¡Ö€Õ¡Õ£Õ«Ö€, Õ¯Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¥Õ¶Ö„\n",
        "\n",
        "* Õ°Õ¡Ö€ÖÕ«Õ¶ Õ°Õ¡Õ¿Õ¯Õ¡ÖÕ¾Õ¡Õ® ÕªÕ¡Õ´Õ¡Õ¶Õ¡Õ¯Õ¨ (total_time)\n",
        "* Õ°Õ¡Ö€ÖÕ« Õ¢Õ¸Õ¾Õ¡Õ¶Õ¤Õ¡Õ¯Õ¸Ö‚Õ©ÕµÕ¡Õ¶ Õ´Õ¥Õ» Õ¥Õ²Õ¡Õ® Õ¢Õ¡Õ¼Õ¥Ö€Õ« Ö„Õ¡Õ¶Õ¡Õ¯Õ¨ (question)\n",
        "* Õ°Õ¡Ö€ÖÕ« ÕºÕ¡Õ¿Õ¡Õ½Õ­Õ¡Õ¶Õ« 4 Õ¿Õ¡Ö€Õ¢Õ¥Ö€Õ¡Õ¯Õ¶Õ¥Ö€Õ« Õ´Õ¥Õ» Õ¡Õ¼Õ¯Õ¡ Õ¢Õ¡Õ¼Õ¥Ö€Õ« Ö„Õ¡Õ¶Õ¡Õ¯Õ¶Õ¥Ö€Õ« Õ£Õ¸Ö‚Õ´Õ¡Ö€Õ¨ (answer1,2,3,4)\n",
        "\n",
        "ÕˆÖ€ÕºÕ¥Õ½ ÕºÕ«Õ¿Õ¡Õ¯ (label) Õ¯Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¥Õ¶Ö„ correct Õ½ÕµÕ¡Õ¶ Õ¿Õ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ¨:\n",
        "\n",
        "ÕŠÕ¡Õ°Õ¡Õ¶Õ»Õ¾Õ¸Ö‚Õ´ Õ§ Õ¬Ö€Õ¡ÖÕ¶Õ¥Õ¬ function6-Õ¨ Õ¡ÕµÕ¶ÕºÕ¥Õ½, Õ¸Ö€ Õ¡ÕµÕ¶ Õ«Ö€ Õ´Õ¥Õ» Õ¯Õ¡Õ¼Õ¸Ö‚ÖÕ« sklearn.pipeline.Pipeline, Õ°Õ¥Õ¿Ö‡ÕµÕ¡Õ¬ ÖƒÕ¸Ö‚Õ¬Õ¥Ö€Õ¸Õ¾â€¤\n",
        "* gender Õ½ÕµÕ¡Õ¶ Õ¾Ö€Õ¡ Õ¯Õ«Ö€Õ¡Õ¼Õ¾Õ¸Õ² sklearn.preprocessing.OrdinalEncoder\n",
        "* question Õ½ÕµÕ¡Õ¶ Õ¾Ö€Õ¡ Õ¯Õ«Ö€Õ¡Õ¼Õ¾Õ¸Õ² sklearn.preprocessing.FunctionTransformer` Õ¡ÕµÕ¶ÕºÕ«Õ½Õ« Ö†Õ¸Ö‚Õ¶Õ¯ÖÕ«Õ¡ÕµÕ¸Õ¾, Õ¸Ö€Õ¸Õ¾ Õ¯Õ¾Õ¥Ö€Õ¡Õ¤Õ¡Ö€Õ±Õ¾Õ« Õ¢Õ¡Õ¼Õ¥Ö€Õ« Ö„Õ¡Õ¶Õ¡Õ¯Õ¨ Õ¡Õ¶Õ¿Õ¥Õ½Õ¥Õ¬Õ¸Õ¾ Õ¿Õ¡Õ¼Õ¥Ö€Õ«Ö Õ¢Õ¡ÖÕ« Õ¡ÕµÕ¬ Õ¶Õ«Õ·Õ¥Ö€Õ¨ [[Õ°Õ¸Ö‚Õ·Õ¸Ö‚Õ´]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
        "* answer1,2,3,4 Õ½ÕµÕ¸Ö‚Õ¶Õ¥Ö€Õ« Õ¾Ö€Õ¡ Õ¯Õ«Ö€Õ¡Õ¼Õ¾Õ¸Õ² sklearn.preprocessing.FunctionTransformer` Õ¡ÕµÕ¶ÕºÕ«Õ½Õ« Ö†Õ¸Ö‚Õ¶Õ¯ÖÕ«Õ¡ÕµÕ¸Õ¾, Õ¸Ö€Õ¸Õ¾ Õ¯Õ¾Õ¥Ö€Õ¡Õ¤Õ¡Ö€Õ±Õ¾Õ« 4 Õ¿Õ¡Ö€Õ¢Õ¥Ö€Õ¡Õ¯Õ¶Õ¥Ö€Õ« Õ´Õ¥Õ» Õ¥Õ²Õ¡Õ® Õ¢Õ¡Õ¼Õ¥Ö€Õ« Ö„Õ¡Õ¶Õ¡Õ¯Õ« (Õ°Õ¡Õ·Õ¾Õ¥Õ¬ Õ¶Õ¡Õ­Õ¸Ö€Õ¤ Õ¯Õ¥Õ¿Õ« Õ¶Õ´Õ¡Õ¶) Õ£Õ¸Ö‚Õ´Õ¡Ö€Õ¨\n",
        "* Õ¡ÕµÕ¶Õ¸Ö‚Õ°Õ¥Õ¿Ö‡ Õ´Õ¸Ö‚Õ¿Ö„Õ¡ÕµÕ«Õ¶ Õ¿Õ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ« (X) Õ¢Õ¸Õ¬Õ¸Ö€ Õ½ÕµÕ¸Ö‚Õ¶Õ¥Ö€Õ« Õ¾Ö€Õ¡ Õ¯Õ«Ö€Õ¡Õ¼Õ¥Õ¬ sklearn.preprocessing.StandardScaler\n",
        "* Ö‡ Õ¸Ö€ÕºÕ¥Õ½ Õ¤Õ¡Õ½Õ¡Õ¯Õ¡Ö€Õ£Õ«Õ¹  Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¥Õ¬ sklearn.neighborsâ€¤KNeighborsClassifier-Õ¨ k=7 Õ¡Ö€ÕªÕ¥Ö„Õ¸Õ¾\n",
        "\n",
        "Ô±Õ¼Õ¡Õ¶Õ±Õ«Õ¶ Õ½ÕµÕ¸Ö‚Õ¶Õ¥Ö€Õ« Õ¾Ö€Õ¡ Õ±Ö‡Õ¡ÖƒÕ¸Õ­Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶ Õ¡Õ¶Õ¥Õ¬Õ¸Ö‚ Õ°Õ¡Õ´Õ¡Ö€ Õ¯Õ¡Ö€Õ¸Õ² Õ¥Ö„ Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¥Õ¬ [Õ½Õ¡](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn-compose-columntransformer)Ö‰ ÕŠÕ¡Õ°Õ¡Õ¶Õ»Õ¾Õ¸Ö‚Õ´ Õ§ Õ¾Õ¥Ö€Õ¡Õ¤Õ¡Ö€Õ±Õ¶Õ¥Õ¬ train (``hw4_bonus_train.csv``) Õ¢Õ¡Õ¦Õ´Õ¸Ö‚Õ©ÕµÕ¡Õ¶ Õ¾Ö€Õ¡ Õ¸Ö‚Õ½Õ¸Ö‚ÖÕ¡Õ¶Õ¾Õ¡Õ® pipeline-Õ« Ö…Õ¢ÕµÕ¥Õ¯Õ¿Õ¨ Ö‡ test (``hw4_bonus_test.csv``) Õ¢Õ¡Õ¦Õ´Õ¸Ö‚Õ©ÕµÕ¡Õ¶ Õ¾Ö€Õ¡ Õ¯Õ¡Õ¶Õ­Õ¡Õ¿Õ¥Õ½Õ¸Ö‚Õ´Õ¶Õ¥Ö€Õ« Õ³Õ·Õ£Ö€Õ¿Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¨ (Õ¯Õ¡Ö€Õ¥Õ¬Õ« Õ§ Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¥Õ¬ score Õ´Õ¥Õ©Õ¸Õ¤Õ¨)Ö‰\n",
        "\n"
      ],
      "metadata": {
        "id": "awSzPs85woW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Õ†Õ¥Ö€Õ¢Õ¥Õ¼Õ¶Õ¥Õ¬ Õ¿Õ¾ÕµÕ¡Õ¬Õ¶Õ¥Ö€Õ¨** ðŸ”½"
      ],
      "metadata": {
        "id": "IflWymnwzpeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/NshanPotikyan/Dasa1Doom/master/files/hw4_bonus.zip\n",
        "!unzip hw4_bonus.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFr62Agfx83f",
        "outputId": "747ec6f3-f09d-4ec2-eefa-b8a8b496824a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-13 11:22:17--  https://raw.githubusercontent.com/NshanPotikyan/Dasa1Doom/master/files/hw4_bonus.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34872 (34K) [application/zip]\n",
            "Saving to: â€˜hw4_bonus.zipâ€™\n",
            "\n",
            "hw4_bonus.zip       100%[===================>]  34.05K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-05-13 11:22:17 (17.2 MB/s) - â€˜hw4_bonus.zipâ€™ saved [34872/34872]\n",
            "\n",
            "Archive:  hw4_bonus.zip\n",
            "  inflating: hw4_bonus_train.csv     \n",
            "  inflating: hw4_bonus_test.csv      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def count_words(D):\n",
        "  X = D.apply(' '.join,axis=1)\n",
        "  vectorizer = CountVectorizer()\n",
        "  X = vectorizer.fit_transform(X)\n",
        "  return np.asarray(X.sum(axis=1).reshape(-1, 1))\n",
        "\n",
        "\n",
        "def function(train,test):\n",
        "\n",
        "  features = ['gender', 'avg_time', 'avg_correct', 'total_time', 'question', 'answer1', 'answer2', 'answer3', 'answer4']\n",
        "  gender_encoder = ('gender_encoder', OrdinalEncoder(), ['gender'])\n",
        "  question_word_count = ('question_word_count', FunctionTransformer(lambda X: count_words(X[['question']])), ['question'])\n",
        "  answer_word_count = ('answer_word_count', FunctionTransformer(lambda X: count_words(X[['answer1', 'answer2', 'answer3', 'answer4']])),['answer1', 'answer2', 'answer3', 'answer4'])\n",
        "\n",
        "  preprocessor = ColumnTransformer(transformers=[\n",
        "      gender_encoder,\n",
        "      question_word_count,\n",
        "      answer_word_count\n",
        "  ], remainder='passthrough')\n",
        "  \n",
        "  pipeline = Pipeline([\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('clf', KNeighborsClassifier(n_neighbors=7))\n",
        "  ])\n",
        "\n",
        "  X_train = train[features]\n",
        "  y_train = train['correct']\n",
        "  pipeline.fit(X_train, y_train)\n",
        "  \n",
        "  X_test = test[features]\n",
        "  y_test = test['correct']\n",
        "  accuracy_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "  return pipeline, accuracy_score"
      ],
      "metadata": {
        "id": "_3wpZ384__0F"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Õ¡ÕµÕ½Õ¿Õ¥Õ² Õ¯Õ¡Ö€Õ¸Õ² Õ¥Ö„ Õ½Õ¿Õ¸Ö‚Õ£Õ¥Õ¬ Õ±Õ¥Ö€ Õ¬Õ¸Ö‚Õ®Õ¸Ö‚Õ´Õ¨\n",
        "train = pd.read_csv('hw4_bonus_train.csv')\n",
        "test = pd.read_csv('hw4_bonus_test.csv')\n",
        "pipeline,accuracy = function(train,test)\n",
        "print('Prediction Accuracy:', accuracy)\n",
        "print('Pipeline: ',pipeline)"
      ],
      "metadata": {
        "id": "w3trqv6njTpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}